# Science in Data Science

![_ds1]({{ site.baseurl }}/images/science_of_ds_1.png)

Data Science, as the name suggests is the science of data, but in practice, everyone found that the so-called "science" side is almost non-existent. The Harvard Business Review claims that data science is the sexiest science of the 21st century [1], but in fact it has become a data technologist and business masseur, not only not sexy, but even beautiful. Obviously there are deep problems and a big problem. It is difficult to fully explain in an article. So let's start with "Science". This article is divided into three parts:
1. First explain what is “science” and its constituent elements;
2. Secondly, the methodology to be followed in order to achieve the conclusions of “science”;
3. Finally, we use an example to illustrate what kind of analysis meets the aforementioned requirements for data science;

## Science and its constituent elements
So what is "science"? The word science in English comes from the Latin _scientia_, which is interpreted in Wikipedia as a framework for systematically constructing and organizing the world's interpretation and prediction through testable methods [2]. The explanation here contains several core elements. First of all, the knowledge system here contains two parts: interpretation and prediction of the surrounding world (natural world and human world); secondly, these contents need to be testable; third, this is a systematic framework for The above information is constructed and organized.
Let us first explain and predict. Interpretation is to explain the phenomenon that occurs from the logic and mechanism. When the mechanism of interpretation is in line with the inevitable laws of physics or social economics behind the phenomenon, it is natural to make correct predictions. If the mechanism behind the phenomenon cannot be logically explained and summarized, then the prediction is essentially the embodiment of the correlation. If the context of the correlation is broad-based, then it is okay. If the basic assumptions are high, then it will often A sudden failure occurs without knowing it, and after the failure, the cause is unknown and can only be followed by the data.
This naturally leads to testable. For science, testable mainly refers to two aspects: one is testable; the other is reproducible.

Classical physics, psychology, biostatistics, and economics all have a number of scientific examples to confirm these arguments.

## Scientific methodology

To follow the scientific framework, then in practice you need to follow the six-step scientific approach, namely:

![_ds2]({{ site.baseurl }}/images/science_of_ds_2.png)

Every step here requires different skills:
1. In the process of finding problems, what is needed is the acumen of the data and the understanding of the problems faced; this is different from the “familiar business” that is often said in China.
2. In the hypothesis phase, a very structured and divergent thinking is needed; structuring refers to a comprehensive and accurate description of the elements of hypothesis; divergence refers to multiple perspectives and multiple dimensions for the consideration of impact factors;
3. In the design experiment phase, you need a very specialized knowledge of experimental design. For example, you need to judge whether the test mean is different or the source of variance, whether the experimental object is homogenous or heterogeneous, and the controllable factor is exogenous. There is still a certain endogeneity, the level of the experimental factors and the relative number of blocks, whether the experimental observations have truncated, whether the truncation is independent or non-independent. According to different judgments, very different processing is required. These tasks need to fully understand the normative analytical framework, conduct correct factor judgments and mappings, and choose the theoretical path that best fits the actual situation. If you can't experiment and do observation analysis, you need to use appropriate data processing to eliminate the interaction between the data. Therefore, it requires a strong professionalism.
4. The data collection phase involves the data burying and log processing of the system. This also requires very professional technology. It is necessary to have a very professional and long-term experience to reasonably predict the information required for hypothesis determination and data analysis and to perform burying and data collection.
5. The Analytical Data Phase is a window in which data scientists' professional skills are displayed in a centralized manner. Whether the data faced can be effectively incorporated into a systematic analysis framework for structured, systematic, and quantitative information deconstruction and reproduction is an important criterion for the _professionalization_ of data scientists.
6. Inferring the conclusion phase requires data scientists to organically integrate examples and statistical information to make recommendations for logical and business mechanisms.

## Some examples
To better illustrate the problem, let's take an example to show. This example is a question that I often ask during an interview. It also reflects good and bad answers. The readers can use these examples to experience the difference between a data scientist trained by systematic science and a wild path analyst.
In 2007, the real estate financial crisis gradually entered the area to be erupted. Many of the first-line business personnel of the Spring River Plumbing Duck Prophet began to jump off the huge financial giant to survive. A warehouse analyst of a real estate finance company found that the bank branch where the general manager left the company also had an accelerated repayment of the stock loan. As a background knowledge, you need to understand that real estate in the United States is a personal financial asset. Mortgage can be used to reinstate the value-added portion of the property for personal use through refinancing (not necessarily the same bank).
Now the boss has found a data scientist and asked what should we do in the face of this situation? Are these accelerated repayments normal? Is the general manager taking the business away? How are you going to answer this question?
I found that most of the so-called data analysts at work and interviews, especially Internet data analysts, are the answer.
The first person will draw a picture first, according to whether there is a manager leaving the job, the proportion of the resignation of the general manager and the repayment of the unit time of the house loan in the branch office. In this data, this graph shows that there are two curves in the division where the general manager leaves the company, which will have a similar trend with increasing time. At this time, the analyst claimed that there should be a correlation between the two. It is recommended to make a highly subsidized refinancing plan for the stock loan of the general manager after the departure of the general manager, so as to strive to maximize the size of the existing loan. Of course, people who answer this question can't pass the interview.
The second person will go a little deeper. They claim that the above picture is too low to be viewed by the naked eye. As an analyst, at least one correlation coefficient is analyzed. These people are still classified into {0,1} data according to whether there is a general manager who leaves the division, and the data is marked as {0,1} for a period of time (for example, from a certain time, the general manager has been transferred to the nearest office.) One week, etc.) The correlation coefficient of the loan repayments of these two types of branch offices was calculated and found to be about 0.372. It was claimed that statistically found a significant correlation, suggesting that the stock loans of these branches should be enlarged. Amount of subsidized refinancing to achieve "guarantee scale." Sorry, please leave. "Don't, I can still dismantle it and see if the correlation between different types of branches is significant...". Still please leave.
The third person said: I am an algorithmic engineer. The above method is too low. I should use the general manager as the independent variable to predict the repayment speed of the loan. You see, there are now 500 branch offices in my hands, 56 of which have the general manager's resignation. I can predict the result very well when I incorporate it into a linear regression. R^2 is about 0.46, which is good. After all, there is only one independent variable, the regression coefficient is 0.34, and the t-test is significantly greater than zero. Although my conclusion is the same as that of the previous students, my method is more rigorous, it is a regression framework, and it is extensible. Sorry, please leave.
The fourth person ran forward and said: I am a senior algorithm engineer. The professional level of the children in front is too low. It is no wonder that something went wrong. This should not be modeled with aggregate data, too much information is averaged out and should be modeled in the dimensions of a single loan. In this way, I can not only include the independent variables of the general manager's departure, but also the attributes of each loan. After all, the natural repayment rate of different loans is different. It seems to be a bit of awkward. Of course, the dependent variable of this is the {0,1} variable of repayment, so this is a classification problem. I intend to do it with the most advanced machine learning model, which is much more advanced than linear regression. I took out millions of stock loans from the warehouse, used all the loan attributes that can be used, and the user label of the lender can also be used, plus whether the general manager of the branch office is resigned. The argument of {0,1} is included in my classifier. I don't use a linear classifier either, that is too low, I use XGBoost, GPU version! State of Art! I also used Variable Importance to determine whether the general manager's departure is an important explanatory variable, and used the PDP to determine whether the average impact of the resignation of this independent variable on the return of the loan is positive or negative? What, do you still need a confidence interval? Sorry, I don't know how to get the confidence interval of the impact value in the XGBoost model, but I know ICE, the full name is called individual conditional expectation, I don't tell him the average person. ICE can plot the performance of the entire data set on this turnover variable, similar to the confidence interval. Sorry, please leave.

Ok, there are four people. The algorithm and data analysis are not right. What are you going to do? Then we analyze each of them in accordance with the six steps listed above.
1. First of all, we observed a phenomenon in which a number of general managers of the branch offices have recently left, and the stock loan repayments of these branches are higher than those of other branch offices;
2. What is our hypothesis? We need to verify whether the resignation of the general manager of the branch office has an accelerating effect on the repayment rate of the stock loan of the branch office; in other words, it is relative to other branch offices, where the general manager leaves the branch office. The remaining time interval of the stock loan in the observation period is less than the retention time interval of the stock loan of the general manager without the separation of the general manager during the observation period; therefore, it is necessary to carry out the experiment in a time to event analysis framework/ Observation analysis
3. How is the framework for experimental design or observational analysis performed in a non-experimental scenario? Because this is a non-experimental scenario, we analyzed it in the framework of observational analysis, with no experimental or control groups. Our observations are the retention intervals of stock loans during the observation period and therefore have a censored phenomenon. At the same time, we learned that during the observation period, whether the general manager of the branch office is not a static variable, but an independent variable that changes with time. If we compress the time, we will endogenously deal with this independent variable according to the final result. That is to say, we can't judge from the data whether the loan repayment is accelerated and the performance is not good, which leads to the departure of the general manager of the branch office. Or the general manager's resignation management chaos caused the loan repayment to accelerate. After all, there was loan repayment before and after the general manager's departure, and both of these cases were theoretically logically reasonable. Remember, the data can only be falsified and cannot be confirmed.
4. Because this is not an experiment, data collection is obtained in the existing system, but we have analyzed the data assumptions from above and found that it is necessary to use the survival analysis with time-variable independent variables to analyze the data. At the time, we have to make reasonable arrangements in the organization of the data and the granularity of the data. Here, we organize all the data according to the granularity of each loan at a given time interval, such as days or weeks. The loan and lender's attribute variables are processed at the corresponding time lines according to whether they change with time. Two types of survival analysis methods can be used for such data. One is to analyze the relative change of the risk ratio, and use the Cox model of time varying covariate to analyze and compare the repayment risk ratio (that is, the repayment rate) of the stock loan of the two types of branch offices of the general manager of the branch office. Relative size; because the data has been discretized, it can be similarly analyzed in the framework of discrete logistic regression; the other can also be analyzed in the framework of the piecewise exponential model for the duration of the observation period.
5. Finally, we will analyze the above regression and, in the case of controlling other factors affecting the loan repayment speed, use the parameter estimates corresponding to the factor of the general manager to leave the job to answer whether their departure will really show up from the data. Accelerate the impact of repayments. If there is such an influence, the model can be used to screen out the additional “accelerated” stock loans to give a certain degree of preferential treatment and slow down the loss. Of course, the actual data shows that the general manager's departure does not have any acceleration impact. This is also a combination of logic. For a C-side housing lender, the management's influence on each business is extremely limited, even if the management is chaotic, as long as the price A low enough user will also reside.
The above mentioned how to conduct standardized analysis in this case, a more detailed analysis readers can refer to the literature [3].

## Conclusion
To sum up, scientific data science must be:
1. Implemented using elements of a standard scientific analytical framework;
2. Abstracted from the actual problem
3. Using a quantitative analytical paradigm with solid theoretical foundations, whether statistical or econometric or quantitative psychology, and testing the fit between the hypothesis of the analytical paradigm and the abstract results.
4. The problems you see must have been encountered and analyzed by someone in the world, Search It!

## Reference materials:
1. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century
2. https://en.wikipedia.org/wiki/Science#cite_note-EOWilson1999a-2
3. Crowley, J. and Hu, M. (1977), “Covariate Analysis of Heart Transplant Survival Data”, Journal of the American Statistical Association, 72, 27-36